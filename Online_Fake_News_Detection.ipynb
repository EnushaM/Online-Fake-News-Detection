{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Online Fake News Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "UEcHMYSWxSsV",
        "HVX8Ok4mxSsV",
        "H5fu3z3ZxSsW",
        "9LhMxfSKxSsW",
        "ok3WkVFJxSsX",
        "OIypGkqcxSsX",
        "CX4mGAwqxSsX",
        "llnifIK3xSsY",
        "vIklj4JDxSsY"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GN22NUdxSsL"
      },
      "source": [
        "![](https://acart.com/wp-content/uploads/2017/05/fake-news-cartoon-zyglis-2.jpg)\n",
        "\n",
        "The spread of misinformation on social media platforms is an ever-growing problem. Organizations, politicians, individuals looking for personal gain and even certain news media outlets engage in propagating fake news to sway people's decisions as well as distorting events to fit a bias or prejudice. \n",
        "\n",
        "The degree of authenticity of the news posted online cannot be definitively measured, since the manual classification of news is tedious and time-consuming and is also subject to bias. \n",
        "\n",
        "To tackle the growing problem, detection, classification and mitigation tools are a need of the hour.\n",
        "\n",
        "# Methodology\n",
        "The categories, bs (i.e. bullshit), junksci(i.e. junk science), hate, fake, conspiracy, bias, satire and state declare the\n",
        "category under which untrustworthy or false news fall under. \n",
        "\n",
        "The first step, which is text preprocessing was performed using the following:\n",
        "* Taking care of null/missing values \n",
        "* Transforming categorical data with the help of label encoders \n",
        "* Uppercase to lowercase \n",
        "* Number removal \n",
        "* Tokenization \n",
        "* Stop Word Removal, Stemming and Lemmatization (with POS tagging) using the Natural Language Toolkit Library \n",
        "\n",
        "For feature engineering, the TF-IDF technique is used. \n",
        "This processed and embedded text is provided as an input to Machine learning models, where the data is made to fit the model, to get a prediction as an output. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "trusted": true,
        "id": "d-Vniz8gxSsP"
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXiFC-f_xSsQ"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Gsq38mAfxSsR"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cufflinks as cf\n",
        "import plotly\n",
        "import plotly.express as px\n",
        "import seaborn as sns\n",
        "\n",
        "from IPython.core.display import HTML\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from datetime import datetime\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from pandas import DataFrame\n",
        "from collections import OrderedDict \n",
        "from colorama import Fore, Back, Style\n",
        "y_ = Fore.YELLOW\n",
        "r_ = Fore.RED\n",
        "g_ = Fore.GREEN\n",
        "b_ = Fore.BLUE\n",
        "m_ = Fore.MAGENTA\n",
        "sr_ = Style.RESET_ALL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4JlypjUxSsR"
      },
      "source": [
        "# Reading the csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "gOkK5lyPxSsR"
      },
      "source": [
        "df = pd.read_csv(r'../input/source-based-news-classification/news_articles.csv', encoding=\"latin\", index_col=0)\n",
        "df = df.dropna()\n",
        "df.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "L_lAg8wxxSsS"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Msphih7IxSsS"
      },
      "source": [
        "df['type'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "vHXCPQQnxSsS"
      },
      "source": [
        "cf.go_offline()\n",
        "cf.set_config_file(offline=False, world_readable=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5t27vACxSsT"
      },
      "source": [
        "# Distrubution of types of articles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "uZv6xpuXxSsT"
      },
      "source": [
        "df['type'].value_counts().plot.pie(figsize = (8,8), startangle = 75)\n",
        "plt.title('Types of articles', fontsize = 20)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nFFAZpUxSsT"
      },
      "source": [
        "# Unigrams and bigrams "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DCLyOwhpxSsU"
      },
      "source": [
        "def get_top_n_words(corpus, n=None):\n",
        "    vec = CountVectorizer().fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "\n",
        "def get_top_n_bigram(corpus, n=None):\n",
        "    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "\n",
        "\n",
        "def get_top_n_trigram(corpus, n=None):\n",
        "    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "kmgIIIVgxSsU"
      },
      "source": [
        "common_words = get_top_n_words(df['text_without_stopwords'], 20)\n",
        "df2 = DataFrame (common_words,columns=['word','count'])\n",
        "df2.groupby('word').sum()['count'].sort_values(ascending=False).iplot(\n",
        "    kind='bar', yTitle='Count', linecolor='black', title='Top 20 unigrams used in articles',color='blue')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "0hG3LM_PxSsU"
      },
      "source": [
        "common_words = get_top_n_bigram(df['text_without_stopwords'], 20)\n",
        "df3 = pd.DataFrame(common_words, columns = ['words' ,'count'])\n",
        "df3.groupby('words').sum()['count'].sort_values(ascending=False).iplot(\n",
        "    kind='bar', yTitle='Count', linecolor='black', title='Top 20 bigrams used in articles', color='blue')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEcHMYSWxSsV"
      },
      "source": [
        "# WordCloud of articles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "zURwV7ZNxSsV"
      },
      "source": [
        "wc = WordCloud(background_color=\"black\", max_words=100,\n",
        "               max_font_size=256,\n",
        "               random_state=42, width=1000, height=1000)\n",
        "wc.generate(' '.join(df['text_without_stopwords']))\n",
        "plt.imshow(wc, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVX8Ok4mxSsV"
      },
      "source": [
        "# Articles including images vs Label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "xvMP5shhxSsV"
      },
      "source": [
        "fig = px.bar(df, x='hasImage', y='label',title='Articles including images vs Label')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "RgAH2tmBxSsV"
      },
      "source": [
        "def convert(path):\n",
        "    return '<img src=\"'+ path + '\" width=\"80\">'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DQVzl3duxSsV"
      },
      "source": [
        "df_sources = df[['site_url','label','main_img_url']]\n",
        "df_r = df_sources.loc[df['label']== 'Real'].iloc[6:10,:]\n",
        "df_f = df_sources.loc[df['label']== 'Fake'].head(6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "2qoisSBJxSsW"
      },
      "source": [
        "HTML(df_r.to_html(escape=False,formatters=dict(main_img_url=convert)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "FdHblJMBxSsW"
      },
      "source": [
        "HTML(df_f.to_html(escape=False,formatters=dict(main_img_url=convert)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "uB2cNlAjxSsW"
      },
      "source": [
        "df['site_url'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "nHxhsV5VxSsW"
      },
      "source": [
        "type_label = {'Real': 0, 'Fake': 1}\n",
        "df_sources.label = [type_label[item] for item in df_sources.label] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "9HGqwWRvxSsW"
      },
      "source": [
        "val_real=[]\n",
        "val_fake=[]\n",
        "\n",
        "for i,row in df_sources.iterrows():\n",
        "    val = row['site_url']\n",
        "    if row['label'] == 0:\n",
        "        val_real.append(val)\n",
        "    elif row['label']== 1:\n",
        "        val_fake.append(val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5fu3z3ZxSsW"
      },
      "source": [
        "> # Websites publishing real news"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-9l5uHy3xSsW"
      },
      "source": [
        "uniqueValues_real = list(OrderedDict.fromkeys(val_real)) \n",
        "\n",
        "print(f\"{y_}Websites publishing real news:{g_}{uniqueValues_real}\\n\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LhMxfSKxSsW"
      },
      "source": [
        "# Websites publishing fake news"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "HMftIRwGxSsX"
      },
      "source": [
        "uniqueValues_fake = list(OrderedDict.fromkeys(val_fake)) \n",
        "print(f\"{y_}Websites publishing fake news:{r_}{uniqueValues_fake}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ok3WkVFJxSsX"
      },
      "source": [
        "# Websites publishing both real and fake news"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_-FQVfzXxSsX"
      },
      "source": [
        "real_set = set(uniqueValues_real) \n",
        "fake_set = set(uniqueValues_fake) \n",
        "\n",
        "print(f\"{y_}Websites publishing both real and fake news:{m_}{real_set & fake_set}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UGEdIfHgxSsX"
      },
      "source": [
        "type1 = {'bias': 0, 'conspiracy': 1,'fake': 2,'bs': 3,'satire': 4, 'hate': 5,'junksci': 6, 'state': 7}\n",
        "df.type = [type1[item] for item in df.type] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "e6hNo-q5xSsX"
      },
      "source": [
        "def plot_bar(df, feat_x, feat_y, normalize=True):\n",
        "    \"\"\" Plot with vertical bars of the requested dataframe and features\"\"\"\n",
        "    \n",
        "    ct = pd.crosstab(df[feat_x], df[feat_y])\n",
        "    if normalize == True:\n",
        "        ct = ct.div(ct.sum(axis=1), axis=0)\n",
        "    return ct.plot(kind='bar', stacked=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIypGkqcxSsX"
      },
      "source": [
        "# Label vs Type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "uZ8E-qFzxSsX"
      },
      "source": [
        "plot_bar(df,'type' , 'label')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "M5fkOn41xSsX"
      },
      "source": [
        "fig = px.sunburst(df, path=['label', 'type'])\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CX4mGAwqxSsX"
      },
      "source": [
        "# Websites and types of news published"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "QMZq7XFQxSsX"
      },
      "source": [
        "df_type = df[['site_url','type']]\n",
        "\n",
        "val_bias=[]\n",
        "val_conspiracy=[]\n",
        "val_fake1=[]\n",
        "val_bs=[]\n",
        "val_satire=[]\n",
        "val_hate=[]\n",
        "val_junksci=[]\n",
        "val_state=[]\n",
        "{'bias': 0, 'conspiracy': 1,'fake': 2,'bs': 3,'satire': 4, 'hate': 5,'junksci': 6, 'state': 7}\n",
        "for i,row in df_type.iterrows():\n",
        "    val = row['site_url']\n",
        "    if row['type'] == 0:\n",
        "        val_bias.append(val)\n",
        "    elif row['type']== 1:\n",
        "        val_conspiracy.append(val)\n",
        "    elif row['type']== 2:\n",
        "        val_fake1.append(val)\n",
        "    elif row['type']== 3:\n",
        "        val_bs.append(val)\n",
        "    elif row['type']== 4:\n",
        "        val_satire.append(val)\n",
        "    elif row['type']== 5:\n",
        "        val_hate.append(val)\n",
        "    elif row['type']== 6:\n",
        "        val_junksci.append(val)\n",
        "    elif row['type']== 7:\n",
        "        val_state.append(val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "c2_PZAkqxSsY"
      },
      "source": [
        "uv_bias = list(OrderedDict.fromkeys(val_bias)) \n",
        "uv_conspiracy = list(OrderedDict.fromkeys(val_conspiracy)) \n",
        "uv_fake = list(OrderedDict.fromkeys(val_fake1)) \n",
        "uv_bs = list(OrderedDict.fromkeys(val_bs)) \n",
        "uv_satire = list(OrderedDict.fromkeys(val_satire)) \n",
        "uv_hate = list(OrderedDict.fromkeys(val_hate)) \n",
        "uv_junksci = list(OrderedDict.fromkeys(val_junksci)) \n",
        "uv_state = list(OrderedDict.fromkeys(val_state)) \n",
        "\n",
        "print(f\"{b_}{type1}\\n\")\n",
        "i=0\n",
        "for lst in (uv_bias,uv_conspiracy,uv_fake,uv_bs,uv_satire, uv_hate,uv_junksci,uv_state): \n",
        "    print(f\"{y_}Source URLs for type:{b_}{i}{r_}{lst}\\n\") \n",
        "    i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llnifIK3xSsY"
      },
      "source": [
        "# Shuffling values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "GIqTWSEvxSsY"
      },
      "source": [
        "df1 = df.sample(frac=1)\n",
        "df1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIklj4JDxSsY"
      },
      "source": [
        "# Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ZTGaHh6hxSsY"
      },
      "source": [
        "y = df1.type\n",
        "\n",
        "x = df1.loc[:,['site_url','text_without_stopwords']]\n",
        "x['source'] = x[\"site_url\"].astype(str) +\" \"+ x[\"text_without_stopwords\"] \n",
        "x = x.drop(['site_url','text_without_stopwords'],axis=1)\n",
        "x = x.source"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngXiNzkkxSsY"
      },
      "source": [
        "Right after preprocessing, the output is a corpus of raw texts that are stripped of stopwords, stemmed and lemmatized. \n",
        "\n",
        "In order to get a sparse matrix of TF/IDF values, the following steps are taken:\n",
        "* Tokenization of texts\n",
        "* Counting of the tokens and\n",
        "* Transforming the raw tokens into TF/IDF values\n",
        "\n",
        "The above steps are done with the help of the TfidfVectorizer, which transforms text to feature vectors that can be used\n",
        "as input to estimators/classifiers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "gK3KJiZZxSsY"
      },
      "source": [
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.30)\n",
        "\n",
        "tfidf_vect = TfidfVectorizer(stop_words = 'english')\n",
        "tfidf_train = tfidf_vect.fit_transform(x_train)\n",
        "tfidf_test = tfidf_vect.transform(x_test)\n",
        "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vect.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "U32MbS1_xSsY"
      },
      "source": [
        "tfidf_vect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-7PEU2M-xSsY"
      },
      "source": [
        "tfidf_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcdMzaH0xSsY"
      },
      "source": [
        "AdaBoost works in iterations with a base classifier to ensure accurate predictions of unusual observations.\n",
        "\n",
        "It works in iterations and within each iteration, incorrect observations are given a higher probability for classification for the next iteration. \n",
        "\n",
        "The AdaBoost implemented here has a Decision Tree Classifier as the base classifier with a max depth of the tree being 10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "LJE_NAy4xSsY"
      },
      "source": [
        "Adab = AdaBoostClassifier(DecisionTreeClassifier(max_depth=10),n_estimators=5,random_state=1)\n",
        "Adab.fit(tfidf_train, y_train)\n",
        "y_pred3 = Adab.predict(tfidf_test)\n",
        "ABscore = metrics.accuracy_score(y_test,y_pred3)\n",
        "print(\"accuracy: %0.3f\" %ABscore)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Z1q36lqLxSsZ"
      },
      "source": [
        "Rando = RandomForestClassifier(n_estimators=100,random_state=0)\n",
        "Rando.fit(tfidf_train,y_train)\n",
        "y_pred1 = Rando.predict(tfidf_test)\n",
        "RFscore = metrics.accuracy_score(y_test,y_pred1)\n",
        "print(\"accuracy:  %0.3f\" %RFscore)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}